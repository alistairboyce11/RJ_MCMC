#!/bin/bash
#SBATCH --job-name=AL_tests
#SBATCH --partition=transcale
#SBATCH --time=72:00:00
###################################
# numbers of nodes. max=40
#SBATCH --nodes=6
# numbers of MPI processes per nodes. max=28
#SBATCH --ntasks-per-node=28
# number of mpi processes per processor. max=14
#SBATCH --ntasks-per-socket=14
##################################################
#SBATCH --cpus-per-task=1
# per node 128Gb, 24 cores (maybe 28). max below is 4Gbqq
#SBATCH --mem-per-cpu=1000mb
#SBATCH -o output_%j.txt
#SBATCH -e stderr_%j.txt
#
printf "==========================================\n"
echo "Job Id        :" $SLURM_JOB_ID
echo "Job name      :" $SLURM_JOB_NAME
echo "Node List     :" $SLURM_JOB_NODELIST
echo "Nb. cores/node:" $SLURM_CPUS_ON_NODE
echo "Nb. tot. cores:" $SLURM_NTASKS
echo "Submit Dir.   :" $SLURM_SUBMIT_DIR
printf "==========================================\n\n"
#
#
#export I_MPI_FABRICS=shm:ofa
export I_MPI_FABRICS_LIST=tcp
export I_MPI_DEBUG=1
#export I_MPI_PROCESS_MANAGER=mpd
#export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so

#
source /softs/intel/l_ics/2017_update4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /softs/intel/l_ics/2017_update4/compilers_and_libraries_2017.4.196/linux/mpi/intel64/bin/mpivars.sh intel64
export INTEL_LICENSE_FILE=/softs/intel/l_ics/license:$INTEL_LICENSE_FILE
#
# export PATH=$PATH:/usr/local/share/gmt-4.5.11/bin
#
# Usually the current directory...
cd $SLURM_SUBMIT_DIR
#pwd
#
make clean
make
#
mpirun -n $SLURM_NTASKS -ppn $SLURM_CPUS_ON_NODE ./run > run_test_${SLURM_JOB_NODELIST}_${SLURM_JOB_ID}.txt
#
exit
